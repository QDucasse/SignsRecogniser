'''
# created 06/10/2019 14:22
# by Q.Ducasse
'''

import cv2
import sklearn
import numpy             as np
import pandas            as pd
import seaborn           as sns
import matplotlib.pyplot as plt
from sklearn                 import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics         import confusion_matrix,accuracy_score
from sklearn.naive_bayes     import GaussianNB
from sklearn.cluster         import KMeans

# ============================================
#       CSV FILE LOADING AND VISUALISATION
# ============================================

## Paths creation and generation
## =============================

path_x_train = './data/x_train_gr_smpl.csv'
path_y_train = './data/y_train_smpl.csv'

## Data import & Preprocessing
## ===========================

def load_base_dataset(x_path,y_path):
    # Import images vector values
    signs = pd.read_csv(x_path, sep=',',na_values='None')
    signs.name = "Signs"
    # Import labels
    labels = pd.read_csv(y_path, sep=',',na_values='None')
    # Link label to vectors
    signs.insert(0,"label",labels.values)
    # Line Randomisation
    signs_rd = signs.sample(frac=1)
    signs_rd.name = "Signs randomized"
    return signs, signs_rd

signs, signs_rd = load_base_dataset(path_x_train,path_y_train)
# Optional: Save again as CSV:
# signs_rd.to_csv('./data/x_train_gr_smpl_rd.csv')

# One label only data sets
# ========================

def create_one_label_dataset(nb):
    '''
    Return a dataset with only the labels nb
    '''
    label_is_correct = (signs['label'] == nb)
    signs_1label = signs[label_is_correct]
    path = './data/signs_label' + str(nb) + '.csv'
    signs_1label.to_csv(path)

def load_one_label_dataset(sample_nb):
    '''
    Load a dataset generated by create_one_label_dataset
    '''
    if ((sample_nb < 0) or (sample_nb > 9)):
        raise Exception('No data sample with that number')
    # Load the csv dataset
    dataframe = pd.read_csv('./data/signs_label' + str(sample_nb) + '.csv')
    # Drop the index column
    dataframe = dataframe.drop('Unnamed: 0', axis = 1)
    return dataframe

def dataset_with_boolean_mask(nb):
    '''
    Return the signs dataframe with labels formatted as:
        0 if label == nb
        1 if label != nb

        NOT WORKING !!!
    '''
    signs_1label = signs
    signs_1label.loc[signs_1label['label'] == nb, ['label']] = '0'
    signs_1label.loc[signs_1label['label'] != nb, ['label']] = '1'
    return signs_1label

def load_boolean_mask(nb):
    path_x_train = './data/x_train_gr_smpl.csv'
    path_labels  = './data/y_train_smpl_' + str(nb) + '.csv'
    signs_1label = pd.read_csv(path_x_train, sep=',',na_values='None')
    labels = pd.read_csv(path_labels, sep=',',na_values='None')
    signs_1label.insert(0,"label",labels.values)
    signs_1label.name = "Signs 1 label"
    return signs_1label


## Data visualisation
## ==================
# Check the first and last rows of the basic + randomised data set
# print(signs.head(n=5))
# print(signs.tail(n=5))
# print(signs_rd.head(n=5))
# print(signs_rd.tail(n=5))
# print(signs['0'])

# Display the number of labels of each kind in the dataset
def plot_labels():
    plt.figure()
    sns.set(style="whitegrid", color_codes=True)
    sns.countplot('label',data=signs_rd)
    plt.show()


## Prepare training/test data
## ==========================

# Separation between feature vector (image itself) and target (label)
def separate_train_test(df,label_class, ratio=0.20):
    cols   = [col for col in df.columns if col!=label_class]
    data   = df[cols]
    target = df[label_class]

    # Separation between training/test data with the given ratio
    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = ratio, random_state = 10)
    return data_train, data_test, target_train, target_test

def normalize_dataset(df):
    return pd.DataFrame(preprocessing.scale(df))

# ============================================
#            NAIVE BAYES BENCHMARK
# ============================================

def gaussian_train_test(df,class_feature):
    print("Running Naive Bayes on dataframe '{0}' with class feature '{1}'".format(df.name,class_feature))
    # separate train and test data
    data_train, data_test, target_train, target_test = separate_train_test(df,class_feature)
    # create an object of the type GaussianNB
    gnb = GaussianNB()
    #train the algorithm on training data and predict using the testing data
    pred = gnb.fit(data_train, target_train).predict(data_test)
    #print the accuracy score of the model
    print("Naive-Bayes accuracy : ", accuracy_score(target_test, pred, normalize = True))
    # create the confusion matrix of the model
    print_cmat(target_test,pred)

def print_cmat(actual,pred):
    print(pd.crosstab(actual,pred,rownames=['Actual'],colnames=['Predicted']))


gaussian_train_test(signs_rd,'label')
# ============================================
#           CORRELATION MATRIX
# ============================================

# Using Pearson Correlation

def store_best_abs_corr_attributes(nb,cor_score = 0.01):
    signs_1label = load_boolean_mask(nb)
    cor = signs_1label.corr()
    cor_target = abs(cor["label"])
    relevant_features = cor_target[cor_target>0.01].sort_values(ascending=False)
    relevant_features.to_csv('./data/best_features_smpl_' + str(nb) + '.csv')
    return relevant_features.head(n=10)

def store_best_attributes():
    for i in range(10):
        print("Finding best attributes for " + str(i))
        store_best_abs_corr_attributes(i)

# Print heatmap
# plt.figure(figsize=(12,10))
# print(signs_label3['label'].corr(signs_label3['0']))
# sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
# plt.show()

# From the created files
# ba = best atributes

# Hardcoded values
ba0 = [1172, 1171, 1468, 1220, 1472, 1221,
       1123, 1469, 1419, 1519, 1124, 1471]

ba1 = [1094, 1046, 1172, 1142,  998, 1190,
       1173,  997,  981, 1045,  950, 1143]

ba2 = [1084, 1132, 1083, 1131, 1082, 1130,
       1036, 1081, 1179, 1035, 1129, 1178]

ba3 = [1696, 1697, 1648, 1649, 1695, 1698,
       1745, 1744, 1713, 1712, 1647, 1650]

ba4 = [1849, 1850, 1848, 1897, 1801, 1802,
       1898, 1800, 1896, 1043, 1847, 1309]

ba5 = [1319, 1367, 1271, 1368, 1270, 1318,
       1320, 1415, 1416, 1222, 1366, 1223]

ba6 = [1186, 1738, 1934, 1737, 1786, 1787,
       1885, 1689, 1983, 1688,  633, 1836]

ba7 = [1168, 1120, 1169, 1119, 1167, 1121,
       1216, 1072, 1071, 1215, 1217, 1118]

ba8 = [1280, 1232, 1328, 1184, 1375, 1376,
       1327, 1470, 1423, 1422, 1471, 1469]

ba9 = [1362, 1410, 1363, 1364, 1411, 1365,
       1412, 1317, 1318, 1413, 1361, 1314]

# Create datasets with the n best features

def best_n_attributes(nb):
    global ba0,ba1,ba2,ba3,ba4,ba5,ba6,ba7,ba8,ba9
    ba_n = ['label'] + ba0[:nb] + ba1[:nb] + ba2[:nb] + ba3[:nb] + ba4[:nb] \
                     + ba5[:nb] + ba6[:nb] + ba7[:nb] + ba8[:nb] + ba9[:nb]
    ba_n = [str(i) for i in ba_n]
    return ba_n

def dataset_best_n_attributes(nb):
    ba_n = best_n_attributes(nb)
    # Filter the dataset
    signs_ba_n = signs[ba_n]
    signs_ba_n.name = "Signs with best " + str(nb) + " attributes"
    # Randomize the dataset
    signs_ba_n_rd = signs_ba_n.sample(frac=1)
    signs_ba_n_rd.name = "Signs with best " + str(nb) + " attributes randomized"
    return signs_ba_n,signs_ba_n_rd

# Create the datasets
signs_ba2,signs_ba2_rd = dataset_best_n_attributes(2)
signs_ba5,signs_ba5_rd = dataset_best_n_attributes(5)
signs_ba10,signs_ba10_rd = dataset_best_n_attributes(10)

# Print the datasets
# print(signs_ba2)
# print(signs_ba5)
# print(signs_ba10)
# Store the datasets
# signs_ba2.to_csv('./data/x_train_gr_smpl_2ba.csv')
# signs_ba5.to_csv('./data/x_train_gr_smpl_5ba.csv')
# signs_ba10.to_csv('./data/x_train_gr_smpl_10ba.csv')

# Run Bayes over the new sets
# gaussian_train_test(signs_ba2_rd,'label')
# gaussian_train_test(signs_ba5_rd,'label')
# gaussian_train_test(signs_ba10_rd,'label')

# ============================================
#            K-MEANS CLUSTERING
# ============================================

# Unsupervised -> Drop 'label' field
signs_ba2_rd_nolab = signs_ba2_rd.drop('label',axis = 1)
signs_ba2_rd_std = normalize_dataset(signs_ba2_rd)

signs_ba5_rd_nolab = signs_ba5_rd.drop('label',axis = 1)
signs_ba5_rd_std = normalize_dataset(signs_ba5_rd)

signs_ba10_rd_nolab = signs_ba10_rd.drop('label',axis = 1)
signs_ba10_rd_std = normalize_dataset(signs_ba10_rd)

def plot_elbow(df,nb_clusters = 11):
    wcss = []
    for i in range(1, nb_clusters):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(df)
        wcss.append(kmeans.inertia_)
    plt.plot(range(1, 11), wcss)
    plt.title('The Elbow Method')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()

# plot_elbow(signs_ba2_rd_std)
# # found 3/4 clusters
# plot_elbow(signs_ba5_rd_std)
# # found 3/4 clusters
# plot_elbow(signs_ba10_rd_std)
# # found 3/4 clusters

n_clusters_elbow = 3
n_clusters = 10

def unsup_kmeans_train_test(df,df_std,n_clusters):
    # Fitting K-Means to the dataset
    kmeans = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 42)
    y_kmeans = kmeans.fit_predict(df_std)
    #beginning of  the cluster numbering with 1 instead of 0
    y_kmeans1=y_kmeans
    y_kmeans1=y_kmeans+1
    # New Dataframe called cluster
    cluster = pd.DataFrame(y_kmeans1)
    # Adding cluster to the Dataset1
    df['cluster'] = cluster
    #Mean of clusters
    kmeans_mean_cluster = pd.DataFrame(round(df.groupby('cluster').mean(),1))
    return kmeans_mean_cluster

kmeans_cluster = KMeans(n_clusters = 10)
kmeans_cluster.fit(signs_ba5_rd_std)
print()
print(kmeans_cluster.labels_[::10])
print("KMeans accuracy : ", accuracy_score(signs_ba5_rd['label'],kmeans_cluster.labels_, normalize = True))
